{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUIb8GEFKvNg",
        "outputId": "793c022e-dfd1-4b35-82c8-29e81b3fd176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers.models.auto import AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "import datasets\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "mBOt13HXK7lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = datasets.load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "\n",
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=2000, test_size=200, seed=42)\n",
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "\n",
        "print(\"\\nDataset loaded and split:\")\n",
        "print(split_datasets)\n",
        "print(\"\\nExample from training set:\")\n",
        "print(split_datasets[\"train\"][1])\n",
        "\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-en-hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMrG5DleLnw-",
        "outputId": "c207f54b-acf6-45e7-d5d6-279d1b1a1390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset loaded and split:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 200\n",
            "    })\n",
            "})\n",
            "\n",
            "Example from training set:\n",
            "{'translation': {'en': 'allowance, project', 'hi': 'परियोजना भत्ता'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "source_lang = \"en\"\n",
        "target_lang = \"hi\"\n",
        "prefix = \"translate English to Hindi: \""
      ],
      "metadata": {
        "id": "TFAywRIiLr0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "UkqSeleXLutW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = split_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "wJuJQlO2LxUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "tCQuqosDL03g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "uEU34dNLL-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU5Z0k8lITiU",
        "outputId": "294e2df9-b88f-42fb-db31-479b4156b46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- English to Hindi Translations ---\n",
            "\n",
            "English: The weather is beautiful today.\n",
            "Hindi: मौसम आज सुंदर है.\n",
            "\n",
            "English: Where is the nearest library?\n",
            "Hindi: कहां सबसे नज़दीकी पुस्तकालय है?\n",
            "\n",
            "English: Can you please help me with this problem?\n",
            "Hindi: आप कृपया मेरी इस समस्या के साथ मदद कर सकते हैं?\n",
            "\n",
            "English: Data science is a fascinating field.\n",
            "Hindi: डाटा विज्ञान एक रोमांचक क्षेत्र है।\n",
            "\n",
            "English: This transformer model translates text from one language to another.\n",
            "Hindi: यह रूपांतरण मॉडल एक भाषा से दूसरे भाषा में पाठ अनुवाद करता है.\n",
            "\n",
            "English: Let's meet tomorrow at the coffee shop.\n",
            "Hindi: चलो कल कॉफी की दुकान पर मिलते हैं.\n",
            "\n",
            "English: What is your favorite book?\n",
            "Hindi: आपकी पसंदीदा किताब क्या है?\n",
            "\n",
            "English: The train will arrive at platform number five.\n",
            "Hindi: ट्रेन 5 मंच पर पहुँच जाएगी ।\n",
            "\n"
          ]
        }
      ],
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "\n",
        "english_sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"Where is the nearest library?\",\n",
        "    \"Can you please help me with this problem?\",\n",
        "    \"Data science is a fascinating field.\",\n",
        "    \"This transformer model translates text from one language to another.\",\n",
        "    \"Let's meet tomorrow at the coffee shop.\",\n",
        "    \"What is your favorite book?\",\n",
        "    \"The train will arrive at platform number five.\"\n",
        "]\n",
        "\n",
        "hindi_translations = translator(english_sentences)\n",
        "print(\"--- English to Hindi Translations ---\\n\")\n",
        "for i in range(len(english_sentences)):\n",
        "    print(f\"English: {english_sentences[i]}\")\n",
        "    print(f\"Hindi: {hindi_translations[i]['translation_text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONCLUSION\n",
        "n conclusion, this experiment successfully demonstrated the application of the Transformer architecture for machine translation from English to Hindi. By fine-tuning a pre-trained model on a parallel dataset, we effectively utilized its\n",
        "\n",
        "encoder-decoder structure and attention mechanisms to translate a variety of unseen sentences. The process adhered to the standard workflow of\n",
        "\n",
        "\n",
        "\n",
        "data collection, pre-processing, and training, culminating in a functional model easily deployed for real-time translation using the Hugging Face pipeline. While the qualitative results were positive, future work should focus on quantitative validation by measuring the\n",
        "\n",
        "\n",
        "BLEU score and further optimizing performance through extensive hyperparameter tuning"
      ],
      "metadata": {
        "id": "x-89l-GXNeRW"
      }
    }
  ]
}